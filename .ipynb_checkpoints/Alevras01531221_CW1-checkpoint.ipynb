{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xHSx8l1tUYN"
   },
   "source": [
    "# Coursework 1 - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Name__: Georgios Alevras <br> __CID__: 01531221 <br> __Date__: 23/02/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"outline\"></a>\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Preliminaries](#preliminaries): Exploratory Data Analysis\n",
    "- [Task 1](#task-1): Regression\n",
    "  - [1.1](#q11) Linear Regression\n",
    "  - [1.2](#q12) Ridge Regression\n",
    "  - [1.3](#q13) Relaxation of Lasso Regression\n",
    "- [Task 2](#task-2): Classification\n",
    "  - [2.1](#q21) k-NN Classifier\n",
    "  - [2.2](#q22) Random Forest \n",
    "  - [2.3](#q23) Support Vector Machine (SVM) \n",
    "- [Task 3](#task-3): Mastery Component \n",
    "  - [3.1](#q31) Logistic Regression and Bagging \n",
    "  - [3.2](#q32) Kernelised SVM Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"preliminaries\"></a>\n",
    "# Preliminaries: Exploratory Data Analysis [^](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All packages used in this coursework are imported here.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the data set found in `./Data/chemistry_samples.csv` using the pandas package and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIC0</th>\n",
       "      <th>SM1_Dz(Z)</th>\n",
       "      <th>GATS1i</th>\n",
       "      <th>NdsCH</th>\n",
       "      <th>NdssC</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>FV1</th>\n",
       "      <th>VFV</th>\n",
       "      <th>FV2</th>\n",
       "      <th>FV3</th>\n",
       "      <th>LC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.661280</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>1.602232</td>\n",
       "      <td>1.994272</td>\n",
       "      <td>0.836488</td>\n",
       "      <td>3.153623</td>\n",
       "      <td>15.893033</td>\n",
       "      <td>-27.724370</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>0.756698</td>\n",
       "      <td>5.506249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.936362</td>\n",
       "      <td>1.154287</td>\n",
       "      <td>1.146997</td>\n",
       "      <td>0.904295</td>\n",
       "      <td>2.948308</td>\n",
       "      <td>5.141095</td>\n",
       "      <td>13.590177</td>\n",
       "      <td>-31.821521</td>\n",
       "      <td>-13.408855</td>\n",
       "      <td>1.161298</td>\n",
       "      <td>6.636791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964144</td>\n",
       "      <td>0.415485</td>\n",
       "      <td>1.481028</td>\n",
       "      <td>2.136585</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>-1.156783</td>\n",
       "      <td>15.989419</td>\n",
       "      <td>-3.699312</td>\n",
       "      <td>2.561525</td>\n",
       "      <td>0.500115</td>\n",
       "      <td>1.563388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.074617</td>\n",
       "      <td>1.417296</td>\n",
       "      <td>0.486216</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.066980</td>\n",
       "      <td>2.610960</td>\n",
       "      <td>7.962046</td>\n",
       "      <td>-16.374439</td>\n",
       "      <td>2.448975</td>\n",
       "      <td>1.481888</td>\n",
       "      <td>6.248432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448569</td>\n",
       "      <td>0.836892</td>\n",
       "      <td>1.951012</td>\n",
       "      <td>0.028318</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>1.851095</td>\n",
       "      <td>22.285266</td>\n",
       "      <td>-9.526361</td>\n",
       "      <td>2.870400</td>\n",
       "      <td>0.649234</td>\n",
       "      <td>3.676796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CIC0  SM1_Dz(Z)    GATS1i     NdsCH     NdssC     MLOGP        FV1  \\\n",
       "0  3.661280   0.658363  1.602232  1.994272  0.836488  3.153623  15.893033   \n",
       "1  3.936362   1.154287  1.146997  0.904295  2.948308  5.141095  13.590177   \n",
       "2  0.964144   0.415485  1.481028  2.136585  0.043679 -1.156783  15.989419   \n",
       "3  2.074617   1.417296  0.486216  0.000908 -0.066980  2.610960   7.962046   \n",
       "4  1.448569   0.836892  1.951012  0.028318 -0.039121  1.851095  22.285266   \n",
       "\n",
       "         VFV        FV2       FV3      LC50  \n",
       "0 -27.724370   0.059355  0.756698  5.506249  \n",
       "1 -31.821521 -13.408855  1.161298  6.636791  \n",
       "2  -3.699312   2.561525  0.500115  1.563388  \n",
       "3 -16.374439   2.448975  1.481888  6.248432  \n",
       "4  -9.526361   2.870400  0.649234  3.676796  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the sample data set using the pandas read_csv method into a pandas DataFrame.\n",
    "df_chemistry_samples = pd.read_csv('./Data/chemistry_samples.csv')\n",
    "# Outputing the first 5 samples (rows) to inspect the data.\n",
    "df_chemistry_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our data is comprised of 10 descriptors (columns 1-10, CIC0 to FV3) and a target variable (column 11) which is the toxicity factor, LC50. We then make sure that the data is 'clean' by ensuring there are no empty / NaN entries, and that all data is of the appropriate type, in this case float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain an array of the data-type of all columns\n",
    "data_type = np.array([df_chemistry_samples.dtypes[i] for i in range(df_chemistry_samples.shape[1])])\n",
    "assert(data_type.all() == float)  # we assert that all columns are of type float\n",
    "assert(df_chemistry_samples.isnull().values.any() == False)  # we assert that there is no empty / NaN entry in any column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assertions return True, thus, we confirm the data set is 'clean'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3sSxTA3yzhN"
   },
   "source": [
    "<a name=\"task-1\"></a>\n",
    "# Task 1: Regression [^](#outline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gY8lFgnyzsF"
   },
   "source": [
    "<a name=\"q11\"></a>\n",
    "\n",
    "## 1.1 Linear Regression [^](#outline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to explain linear rigression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_likelihood_estimate(X, y):\n",
    "    # X: matrix containing the descriptor variables, dimensions: N x p\n",
    "    # y: vector containing target observed variable, dimensions: N x 1\n",
    "    # returns: beta_ml -> maximum likelihood estimate parameters, dimensions:  p x 1\n",
    "    \n",
    "    X_aug = np.hstack([np.ones((X.shape[0],1)), X])  # augment matrix X to include constant terms\n",
    "    beta_ml = np.linalg.solve(X_aug.T @ X_aug, X_aug.T @ y)  # explicit minimisation to obtain vector of parameters\n",
    "    return beta_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true, y_predicted):\n",
    "    # y_true: ground truth target values\n",
    "    # y_predicted: predicted / estimated target values\n",
    "    # returns: R2 score, i.e., coefficient of determination regression score function\n",
    "\n",
    "    sum_of_squares_of_residuals = np.array((y_true - y_predicted)**2).sum()  # obtain the residual sum of squares\n",
    "    total_sum_of_squares = np.array((y_true - np.mean(y_true))**2).sum()  # obtain the total sum of squares\n",
    "    return 1 - sum_of_squares_of_residuals/total_sum_of_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.61638041e+00  4.47138333e-02  1.25871884e+00 -3.80092766e-02\n",
      "  3.63073448e-01  4.66534885e-03  3.90510052e-01 -7.46028629e-02\n",
      " -3.57069460e-02 -1.52588258e-02 -1.80315940e-03]\n"
     ]
    }
   ],
   "source": [
    "X = df_chemistry_samples.iloc[:, 0:10]  # load descriptors into matrix of descriptors\n",
    "X_aug = np.hstack([np.ones((X.shape[0],1)), X])  # augment matrix of descriptors to include constant terms\n",
    "y = df_chemistry_samples.iloc[:, 10]  # load ground truth target variable vector\n",
    "\n",
    "# Get the maximum likelihood estimate parameters\n",
    "beta_ml = maximum_likelihood_estimate(X,y)\n",
    "print(beta_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_estimate(X, beta):\n",
    "    # X: matrix containing the descriptor variables\n",
    "    # beta: vector of parameters\n",
    "    # returns: prediction of f(X)\n",
    "    \n",
    "    return X @ beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data R2-score:  0.8718259975718016\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predict_using_estimate(X_aug, beta_ml)  # Obtain predicted target values\n",
    "r2_score_sample = r2_score(y, y_predicted)  # Compute R2 score for ground truth and predicted target variable\n",
    "print('Sample data R2-score: ', r2_score_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data R2-score:  0.8642933369927286\n"
     ]
    }
   ],
   "source": [
    "# Loading the test data set using the pandas read_csv method into a pandas DataFrame.\n",
    "df_chemistry_test = pd.read_csv('./Data/chemistry_test.csv')\n",
    "\n",
    "X_test = df_chemistry_test.iloc[:, 0:10]  # load descriptors into matrix of descriptors\n",
    "X_aug_test = np.hstack([np.ones((X_test.shape[0],1)), X_test])  # augment matrix of descriptors to include constant terms\n",
    "\n",
    "y_test = df_chemistry_test.iloc[:, 10]  # load ground truth target variable vector\n",
    "predicted_y_test = predict_using_estimate(X_aug_test, beta_ml)  # Obtain predicted target values\n",
    "\n",
    "r2_score_test = r2_score(y_test, predicted_y_test)  # Compute R2 score for ground truth and predicted target variable\n",
    "print('Test data R2-score: ', r2_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpbJPhupy0Cj"
   },
   "source": [
    "<a name=\"q12\"></a>\n",
    "## 1.2 Ridge Regression [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to explain ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_estimate(X, y, penalty):\n",
    "    # X: matrix containing the descriptor variables, dimensions: N x p\n",
    "    # y: vector containing target observed variable, dimensions: N x 1\n",
    "    # returns: beta_ridge -> maximum likelihood estimate parameters, dimensions:  p x 1\n",
    "    \n",
    "    N, D = X.shape  # dimensions of matrix X\n",
    "    X_aug = np.hstack([np.ones((N,1)), X])  # augment matrix X to include constant terms\n",
    "    N_aug, D_aug = X_aug.shape  # dimensions of augmented matrix X_aug\n",
    "    I = np.identity(D_aug)  # identity matrix of dimensions D_aug x D_aug\n",
    "    # To improve the shrinkage properties of Ridge Regression so that it is not dominated by large values of the intercept of the linear model\n",
    "    I[0,0] = 0\n",
    "    # Explicit minimisation to obtain vector of parameters\n",
    "    beta_ridge = np.linalg.solve(X_aug.T @ X_aug + penalty * I, X_aug.T @ y)\n",
    "    return beta_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the penalty hyper-parameter using 5-fold cross-validation, we want to shuffle our data (rows in $\\boldsymbol{X}$ and $\\boldsymbol{y}$), create 5 sets (folds) where 4 are used for training and 1 is used for testing. The ridge regression model is run for all combinations of training and test sets over the 5 sets and the average mean squared errors, $\\langle\\text{MSE}\\rangle$, are obtained over all folds. The penalty term is varied to see how the $\\langle\\text{MSE}\\rangle$ changes, aiming to find the optimum penalty term, i.e., the one that minimises the $\\langle\\text{MSE}\\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "y_copy = y.copy()\n",
    "\n",
    "X_copy = np.array(X_copy[:-1]) # removing the last element in order to have 5 equal-sized folds (which requires an even number of samples)\n",
    "y_copy = np.array(y_copy[:-1]) # removing the last element in order to have 5 equal-sized folds (which requires an even number of samples)\n",
    "\n",
    "np.random.seed(10)\n",
    "p = np.random.permutation(len(y_copy))\n",
    "X_copy = X_copy[p]\n",
    "y_copy = y_copy[p]\n",
    "\n",
    "# We obtain a list of the 5 index arrays, i.e., the 5 folds\n",
    "folds_indexes = np.split(np.arange(len(y_copy)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_true, y_predicted):\n",
    "    return np.array((y_true - y_predicted)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_variance(y_predicted):\n",
    "    total_sum_of_squares = np.array((y_true - np.mean(y_true))**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_cross_validation_score(X, y, folds, penalty):\n",
    "    scores = []\n",
    "    for i in range(len(folds)):\n",
    "        validation_fold_indexes = folds[i]\n",
    "        training_fold_indexes = list(set(range(y.shape[0])) - set(validation_fold_indexes))\n",
    "        \n",
    "        X_training = X[training_fold_indexes, :]\n",
    "        y_training = y[training_fold_indexes]\n",
    "\n",
    "        X_validation = X[validation_fold_indexes, :]\n",
    "        y_validation = y[validation_fold_indexes]\n",
    "        \n",
    "        beta_ridge = ridge_estimate(X_training, y_training, penalty)\n",
    "        X_validation_aug = np.hstack([np.ones((X_validation.shape[0], 1)), X_validation])\n",
    "        y_predicted = predict_using_estimate(X_validation_aug, beta_ridge)\n",
    "        scores.append(mean_square_error(y_validation, y_predicted))\n",
    "\n",
    "    # Return the average score\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_choose_best_penalty(X, y, folds, penalty_range):\n",
    "  penalty_scores = np.zeros((len(penalty_range),))\n",
    "  \n",
    "  for i, p in enumerate(penalty_range):\n",
    "    penalty_scores[i] = ridge_cross_validation_score(X, y, folds, p)\n",
    "\n",
    "  best_penalty_index = np.argmin(penalty_scores)\n",
    "  return penalty_scores, penalty_range[best_penalty_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty term: 0.752\n"
     ]
    }
   ],
   "source": [
    "penalty_scores_ridge, best_penalty_ridge = ridge_choose_best_penalty(X_copy, y_copy, folds_indexes, np.linspace(0, 2, 1001))\n",
    "\n",
    "plt.plot(np.linspace(0, 2, 1001), penalty_scores_ridge)\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel(r'$\\langle MSE \\rangle$')\n",
    "plt.show()\n",
    "print('Best penalty term:', best_penalty_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Sample data R2-score:  0.8718249167946791\n",
      "Ridge Test data R2-score:  0.8648403944644114\n"
     ]
    }
   ],
   "source": [
    "beta_ridge = ridge_estimate(X, y, best_penalty_ridge)\n",
    "predicted_y = predict_using_estimate(X_aug, beta_ridge)\n",
    "r2_score_sample_ridge = r2_score(y, predicted_y)  # Compute R2 score for ground truth and predicted target variable\n",
    "print('Ridge Sample data R2-score: ', r2_score_sample_ridge)\n",
    "\n",
    "beta_ridge_test = ridge_estimate(X_test, y_test, best_penalty_ridge)\n",
    "predicted_y_test = predict_using_estimate(X_aug_test, beta_ridge_test)\n",
    "r2_score_test_ridge = r2_score(y_test, predicted_y_test)  # Compute R2 score for ground truth and predicted target variable\n",
    "print('Ridge Test data R2-score: ', r2_score_test_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9OTrzYly0Oz"
   },
   "source": [
    "<a name=\"q13\"></a>\n",
    "\n",
    "## 1.3 Relaxation of Lasso Regression [^](#outline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jg2TWPoT0J0Z"
   },
   "outputs": [],
   "source": [
    "def huber(beta, c=1e-6):\n",
    "    if abs(beta) <= c:\n",
    "        return 0.5*beta**2\n",
    "    else:\n",
    "        return c*(abs(beta) - 0.5*c)\n",
    "\n",
    "    \n",
    "def grad_huber(beta, c=1e-6):\n",
    "    if abs(beta) <= c:\n",
    "        return beta\n",
    "    else:\n",
    "        return np.sign(beta)*c\n",
    "\n",
    "vectorized_grad_huber = np.vectorize(grad_huber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U7hMrAxK0LQN"
   },
   "outputs": [],
   "source": [
    "def minimize_ls_huber(X, y, lambd, n_iters=10000, step_size=1e-3):\n",
    "    n, p = X.shape\n",
    "    XX = X.T @ X / n\n",
    "    Xy = (X.T @ y / n).reshape(-1, 1)\n",
    "    \n",
    "    # next line: initialise betas\n",
    "    beta = np.zeros(shape=(p, 1))\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        grad = -2 * Xy + 2 * XX @ beta + lambd * vectorized_grad_huber(beta)\n",
    "        # next line: gradient descent update\n",
    "        beta = beta - step_size*grad\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAJBCAYAAABiXzB4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbsElEQVR4nO3dfZBlB1nn8d8TMhLYsJFNBowZ4mTLWIsQd0JG0A1LBReX1yIsC0WQUnlNlWtAQasS3RVTVu1WUCuoBStGXiSWBFdBTJa4ihskcVcwLwRCSIDwJgMRh4CBGEIIPPtHd9hJmGS65+mZ2935fKq6cvucc+99Tma659vnnHu7ujsAAOyfQxY9AADARiamAAAGxBQAwICYAgAYEFMAAANiCgBg4NBFPfFRRx3V27dvX9TTAwCs2JVXXvmF7t66t3ULi6nt27fniiuuWNTTAwCsWFV9+p7WOc0HADAgpgAABsQUAMDAwq6ZAgDWp69//evZtWtXbrvttkWPctAddthh2bZtW7Zs2bLi+4gpAOAudu3alQc96EHZvn17qmrR4xw03Z2bbropu3btynHHHbfi+znNBwDcxW233ZYjjzzyPhVSSVJVOfLII1d9RE5MAQDf5r4WUnfan/0WUwDAunP44YcveoQVc80UAHCvtp/1zjV9vE+d89Q1fbxFc2QKANgQLrroojzmMY/JiSeemCc84Qn5/Oc/nyR5z3vekx07dmTHjh058cQT85WvfCU33nhjHve4x2XHjh155CMfmcsuuyxJcsEFF+SEE07IIx/5yJx55plrMpeYAgA2hMc+9rF573vfm/e///057bTT8qu/+qtJkl//9V/Pa1/72lx99dW57LLL8oAHPCBvectb8sQnPjFXX311PvCBD2THjh353Oc+lzPPPDOXXHJJrr766lx++eV5xzveMZ7LaT4AYEPYtWtXnvOc5+TGG2/M7bff/q23Lzj55JPzile8Is973vPyzGc+M9u2bcsP/uAP5oUvfGG+/vWv5xnPeEZ27NiRSy65JKecckq2bl36fcXPe97zcumll+YZz3jGaC5HpgCADeGlL31pzjjjjFxzzTX5nd/5nW+9hcFZZ52V17/+9fnqV7+ak08+Oddff30e97jH5dJLL80xxxyT5z//+Tn//PMP2FyOTAEAG8LNN9+cY445Jkny5je/+VvLP/7xj+eEE07ICSeckMsvvzzXX399HvCAB2Tbtm15yUtekq997Wu56qqrcuaZZ+ZlL3tZvvCFL+TBD35wLrjggrz0pS8dzyWmAIB159Zbb822bdu+9fkrXvGKnH322Xn2s5+dBz/4wfmRH/mRfPKTn0yS/MZv/Ebe/e5355BDDskjHvGIPPnJT85b3/rW/Nqv/Vq2bNmSww8/POeff36OPvronHPOOXn84x+f7s5Tn/rUnHrqqeNZq7vHD7I/du7c2VdcccVCnhsAuGfXXXddHv7why96jIXZ2/5X1ZXdvXNv27tmCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMDA5n+fqbOPWOX2Nx+YOQCAFTv88MNzyy233GXZ2Wefnd/93d/N1q1bc/vtt+eXfumX8tznPndBE/5/mz+mAICZ1R6Y2Ofj7f+Bi5e//OX5+Z//+XzsYx/LSSedlGc961nZsmXLGg63ek7zAQAbzvHHH58HPvCB+dKXvrToUcQUALDxXHXVVTn++OPzkIc8ZNGjOM0HAGwcr371q/OmN70pH/3oR3PRRRctepwkjkwBABvIy1/+8lx77bV529velhe96EW57bbbFj2SmAIANp6nP/3p2blzZ9785jcvehQxBQCsP7feemu2bdv2rY9zzz3327Z55StfmXPPPTff/OY3FzDh/+eaKQDg3i3gPRhXEkgnnXRSPvKRjxyEae6dI1MAAANiCgBgQEwBAAyIKQDg23T3okdYiP3ZbzEFANzFYYcdlptuuuk+F1TdnZtuuimHHXbYqu7n1XwAwF1s27Ytu3btyu7duxc9ykF32GGHZdu2bau6j5gCAO5iy5YtOe644xY9xobhNB8AwMA+Y6qqHlZV766qD1fVtVX1M3vZ5pSqurmqrl7+eOWBGRcAYH1ZyWm+O5L8XHdfVVUPSnJlVb2ruz98t+0u6+6nrf2IAADr1z6PTHX3jd191fLtryS5LskxB3owAICNYFXXTFXV9iQnJnnfXlb/cFV9oKr+rKoesRbDAQCsdyt+NV9VHZ7kbUl+tru/fLfVVyX5nu6+paqekuQdSY7fy2OcnuT0JDn22GP3d2YAgHVjRUemqmpLlkLqD7r77Xdf391f7u5blm9fnGRLVR21l+3O6+6d3b1z69atw9EBABZvJa/mqyRvSHJdd597D9t81/J2qapHLz/uTWs5KADAerSS03wnJ/nxJNdU1dXLy34xybFJ0t2vS/KsJD9VVXck+WqS0/q+9h70AMB90j5jqrv/OkntY5vXJHnNWg0FALBReAd0AIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgYJ8xVVUPq6p3V9WHq+raqvqZvWxTVfVbVXVDVX2wqh51YMYFAFhfDl3BNnck+bnuvqqqHpTkyqp6V3d/eI9tnpzk+OWPxyT57eX/AgBsavs8MtXdN3b3Vcu3v5LkuiTH3G2zU5Oc30vem+Q7q+roNZ8WAGCdWdU1U1W1PcmJSd53t1XHJPnMHp/vyrcHFwDAprPimKqqw5O8LcnPdveX9+fJqur0qrqiqq7YvXv3/jwEAMC6sqKYqqotWQqpP+jut+9lk88medgen29bXnYX3X1ed+/s7p1bt27dn3kBANaVlbyar5K8Icl13X3uPWx2YZKfWH5V3w8lubm7b1zDOQEA1qWVvJrv5CQ/nuSaqrp6edkvJjk2Sbr7dUkuTvKUJDckuTXJC9Z8UgCAdWifMdXdf52k9rFNJ/nptRoKAGCj8A7oAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAwD5jqqreWFX/UFUfuof1p1TVzVV19fLHK9d+TACA9enQFWzze0lek+T8e9nmsu5+2ppMBACwgezzyFR3X5rkiwdhFgCADWetrpn64ar6QFX9WVU9Yo0eEwBg3VvJab59uSrJ93T3LVX1lCTvSHL83jasqtOTnJ4kxx577Bo8NQDAYo2PTHX3l7v7luXbFyfZUlVH3cO253X3zu7euXXr1ulTAwAs3Dimquq7qqqWbz96+TFvmj4uAMBGsM/TfFV1QZJTkhxVVbuS/HKSLUnS3a9L8qwkP1VVdyT5apLTursP2MQAAOvIPmOqu5+7j/WvydJbJwAA3Od4B3QAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYOHTRA3AfdPYR+3Gfm9d+DgBYA45MAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYODQRQ+wGtvPeueq7/Opww7AIAAAyxyZAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAwD5jqqreWFX/UFUfuof1VVW/VVU3VNUHq+pRaz8mAMD6tJIjU7+X5En3sv7JSY5f/jg9yW/PxwIA2Bj2GVPdfWmSL97LJqcmOb+XvDfJd1bV0Ws1IADAerYW10wdk+Qze3y+a3kZAMCmd1AvQK+q06vqiqq6Yvfu3QfzqQEADoi1iKnPJnnYHp9vW172bbr7vO7e2d07t27dugZPDQCwWGsRUxcm+YnlV/X9UJKbu/vGNXhcAIB179B9bVBVFyQ5JclRVbUryS8n2ZIk3f26JBcneUqSG5LcmuQFB2pYAID1Zp8x1d3P3cf6TvLTazYRAMAGss+Ygnuz/ax3rvo+nzrsAAwCAAvi18kAAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGFhRTFXVk6rqI1V1Q1WdtZf1z6+q3VV19fLHi9d+VACA9efQfW1QVfdL8tokP5pkV5LLq+rC7v7w3Tb9w+4+4wDMCACwbq3kyNSjk9zQ3Z/o7tuTvDXJqQd2LACAjWElMXVMks/s8fmu5WV39x+r6oNV9cdV9bC9PVBVnV5VV1TVFbt3796PcQEA1pe1ugD9oiTbu/sHkrwryZv3tlF3n9fdO7t759atW9foqQEAFmclMfXZJHseadq2vOxbuvum7v7a8qevT3LS2owHALC+rSSmLk9yfFUdV1XfkeS0JBfuuUFVHb3Hp09Pct3ajQgAsH7t89V83X1HVZ2R5M+T3C/JG7v72qr6lSRXdPeFSV5WVU9PckeSLyZ5/gGcGQDYpLaf9c5V3+dT5zz1AEyycvuMqSTp7ouTXHy3Za/c4/YvJPmFtR0NAGD98w7oAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAwIreZwqAmY34RoTAyjgyBQAwIKYAAAbEFADAgJgCABgQUwAAA17NB5vQal855lVjAPtPTAGsV2cfsR/3uXnt5wDuldN8AAADYgoAYEBMAQAMiCkAgAEXoAMudAYYcGQKAGBATAEADDjNtyDeVBEANgdHpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAgUMXPQDAvdl+1jtXtf2nznnqAZoEYO8cmQIAGBBTAAADYgoAYEBMAQAMuAB9ozj7iP24z81rPwcAcBeOTAEADIgpAIABMQUAMLCimKqqJ1XVR6rqhqo6ay/r719Vf7i8/n1VtX3NJwUAWIf2GVNVdb8kr03y5CTfn+S5VfX9d9vsRUm+1N3fm+TVSV611oMCAKxHK3k136OT3NDdn0iSqnprklOTfHiPbU5Ncvby7T9O8pqqqu7uNZwVANiL1f7apcSvXlpLta/eqapnJXlSd794+fMfT/KY7j5jj20+tLzNruXPP768zRfu9linJzk9SY499tiTPv3pT6/lvsDIfn0zOuzHVncHb1fBJrCZvlZW/bsfV7sfyfr9ut9Mb7lzEPalqq7s7p17W3dQ32equ89Lcl6S7Ny501ErAGBuwZG3kpj6bJKH7fH5tuVle9tmV1UdmuSIJDetyYQArCv7dXro7DUfA9aNlcTU5UmOr6rjshRNpyW5+3HOC5P8ZJK/SfKsJJe4XgqA9W7VYXj2ARljMdbrKbsNaJ8x1d13VNUZSf48yf2SvLG7r62qX0lyRXdfmOQNSX6/qm5I8sUsBRcAwKa3omumuvviJBffbdkr97h9W5Jnr+1oAADrn3dABwAYOKiv5oNNxzUHcN/ia569cGQKAGBATAEADIgpAIABMQUAMCCmAAAGxBQAwIC3RgDgwPOWAmxijkwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAIABMQUAMHDoogeA9eJT5zx10SMAsAE5MgUAMCCmAAAGxBQAwICYAgAYEFMAAANiCgBgQEwBAAyIKQCAATEFADAgpgAABsQUAMCAmAIAGBBTAAADYgoAYEBMAQAMiCkAgAExBQAwIKYAAAbEFADAgJgCABgQUwAAA2IKAGBATAEADIgpAICB6u7FPHHV7iSfPghPdVSSLxyE5znQNst+JPZlPdos+5HYl/Vqs+zLZtmPxL6s1vd099a9rVhYTB0sVXVFd+9c9BxTm2U/EvuyHm2W/Ujsy3q1WfZls+xHYl/WktN8AAADYgoAYOC+EFPnLXqANbJZ9iOxL+vRZtmPxL6sV5tlXzbLfiT2Zc1s+mumAAAOpPvCkSkAgANGTAEADIgpAICBQxc9wFqqqn+V5NQkxywv+mySC7v7usVNxfKfyzFJ3tfdt+yx/End/b8WN9nqVNWjk3R3X15V35/kSUmu7+6LFzzaWFWd390/seg5pqrqsUkeneRD3f0Xi55nNarqMUmu6+4vV9UDkpyV5FFJPpzkv3X3zQsdcIWq6mVJ/qS7P7PoWaaq6juSnJbkc939l1X1Y0n+TZLrkpzX3V9f6ICrVFX/MskzkzwsyTeSfDTJW7r7ywsdbBPYNBegV9WZSZ6b5K1Jdi0v3palL4S3dvc5i5ptLVXVC7r7TYueY6WWv7H+dJa++exI8jPd/afL667q7kctcLwVq6pfTvLkLP0A8q4kj0ny7iQ/muTPu/u/LnC8VamqC+++KMnjk1ySJN399IM+1H6qqr/t7kcv335Jlv6u/UmSf5/koo30dV9V1yb51919R1Wdl+TWJH+c5N8tL3/mQgdcoaq6Ock/Jfl4kguS/FF3717sVPunqv4gS1/zD0zyj0kOT/L2LP2ZVHf/5OKmW53l78VPS3JpkqckeX+W9uk/JPlP3f1XCxtuE9hMMfXRJI+4+08Kyz9ZXNvdxy9msrVVVX/X3ccueo6Vqqprkvxwd99SVduz9I/D73f3b1bV+7v7xMVOuDLL+7Ejyf2T/H2SbXscQXhfd//AIudbjaq6KktHO16fpLMUUxdk6QePdPd7Fjfd6uz5d6iqLk/ylO7eXVX/LMl7u/uExU64clV1XXc/fPn2XX7QqKqru3vHwoZbhap6f5KTkjwhyXOSPD3JlVn6O/b27v7KAsdblar6YHf/QFUdmqUzHd/d3d+oqkrygQ32dX9Nkh3L8z8wycXdfUpVHZvkTzfK9+IkqaojkvxCkmckeUiWvo/9Q5I/TXJOd//jwZ5pM10z9c0k372X5Ucvr9swquqD9/BxTZKHLnq+VTrkzlN73f2pJKckeXJVnZulf8Q3iju6+xvdfWuSj995WLy7v5oN9vcryc4s/eP2n5PcvPwT6Ve7+z0bKaSWHVJVD66qI7P0w+HuJOnuf0pyx2JHW7UPVdULlm9/oKp2JklVfV+SjXQ6qbv7m939F939oix9X/7vWTot/onFjrZqhyz/QP6gLB2dOmJ5+f2TbFnYVPvvzkt77p+lo2zp7r/LxtuX/5HkS0lO6e5/0d1HZuno+peW1x10m+maqZ9N8r+r6mNJ7jxXf2yS701yxqKG2k8PTfLELP3F2FMl+b8Hf5yRz1fVju6+OkmWj1A9Lckbk2yYowZJbq+qBy7H1El3Llz+CWlDxVR3fzPJq6vqj5b/+/ls3O8FR2QpDCtJV9XR3X1jVR2ejRXrSfLiJL9ZVf8lS7+w9W+q6jNZ+n724oVOtjp3+f++fLbgwiQXLh8R2UjekOT6JPfL0g8ff1RVn0jyQ1m6pGQjeX2Sy6vqfUn+bZJXJUlVbU3yxUUOth+2d/er9lzQ3X+f5FVV9cJFDLRpTvMlSVUdkqWLT/e8AP3y7v7G4qZavap6Q5I3dfdf72XdW7r7xxYw1n6pqm1ZOqrz93tZd3J3/58FjLVqVXX/7v7aXpYfleTo7r5mAWOtiap6apKTu/sXFz3LWln+R/uh3f3JRc+yWlX1z5Mcl6XA3dXdn1/wSKtSVd/X3R9d9Bxrpaq+O0m6+3NV9Z1ZOn35d939twsdbD9U1SOSPDxLL9C4ftHz7K+q+oskf5nkzXd+fVTVQ5M8P8mPdvcTDvpMmymmAIDNraoenKVXu56apWumkuTzWToCek533/2szoGfSUwBAJvBol7xLqYAgE1hUa9436gXnQIA90FV9cF7WpUFveJdTAEAG8m6e8W7mAIANpL/meTwO99yZ09V9VcHfZq4ZgoAYGQzvQM6AMBBJ6YAAAbEFADAgJgCABgQUwAAA/8PXvT6XhtUjgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta_lasso = minimize_ls_huber(X_aug, y, 0, n_iters=10000, step_size=1e-3)\n",
    "\n",
    "beta_l = pd.DataFrame(beta_lasso)\n",
    "beta_m = pd.DataFrame(beta_ml)\n",
    "conc = pd.concat([beta_l, beta_m], axis=1)\n",
    "conc.columns = ['Lasso', 'LR']\n",
    "conc.plot.bar(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_cross_validation_score(X, y, folds, penalty):\n",
    "    scores = []\n",
    "    for i in range(len(folds)):\n",
    "        validation_fold_indexes = folds[i]\n",
    "        training_fold_indexes = list(set(range(y.shape[0])) - set(validation_fold_indexes))\n",
    "        \n",
    "        X_training = X[training_fold_indexes, :]\n",
    "        y_training = y[training_fold_indexes]\n",
    "\n",
    "        X_validation = X[validation_fold_indexes, :]\n",
    "        y_validation = y[validation_fold_indexes]\n",
    "        \n",
    "        X_training_aug = np.hstack([np.ones((X_training.shape[0],1)), X_training])\n",
    "        beta_lasso = minimize_ls_huber(X_training_aug, y_training, penalty)\n",
    "        \n",
    "        X_validation_aug = np.hstack([np.ones((X_validation.shape[0], 1)), X_validation])\n",
    "        y_predicted = predict_using_estimate(X_validation_aug, beta_lasso)\n",
    "        \n",
    "        scores.append(mean_square_error(y_validation, y_predicted))\n",
    "\n",
    "    # Return the average score\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_choose_best_penalty(X, y, folds, penalty_range):\n",
    "    penalty_scores = np.zeros((len(penalty_range),))\n",
    "    \n",
    "    for i, p in enumerate(penalty_range):\n",
    "        penalty_scores[i] = lasso_cross_validation_score(X, y, folds, p)\n",
    "    \n",
    "    best_penalty_index = np.argmin(penalty_scores)\n",
    "    return penalty_scores, penalty_range[best_penalty_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty term: 1.0\n"
     ]
    }
   ],
   "source": [
    "penalty_scores_lasso, best_penalty_lasso = lasso_choose_best_penalty(X_copy, y_copy, folds_indexes, np.linspace(0, 1, 11))\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 11), penalty_scores_lasso)\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel(r'$\\langle MSE \\rangle$')\n",
    "plt.show()\n",
    "print('Best penalty term:', best_penalty_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Sample data R2-score:  0.8308768771732158\n",
      "Lasso Test data R2-score:  0.8233683221899653\n"
     ]
    }
   ],
   "source": [
    "beta_lasso = minimize_ls_huber(X_aug, y, best_penalty_lasso)\n",
    "predicted_y_lasso = predict_using_estimate(X_aug, beta_lasso)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "r2_score_sample_lasso = r2_score(y, predicted_y_lasso)  # Compute R2 score for ground truth and predicted target variable\n",
    "print('Lasso Sample data R2-score: ', r2_score_sample_lasso)\n",
    "\n",
    "beta_lasso_test = minimize_ls_huber(X_aug_test, y_test, best_penalty_lasso)\n",
    "predicted_y_lasso_test = predict_using_estimate(X_aug_test, beta_lasso_test)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "r2_score_test_lasso = r2_score(y_test, predicted_y_lasso_test)  # Compute R2 score for ground truth and predicted target variable\n",
    "print('Lasso Test data R2-score: ', r2_score_test_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi0I8ERdy0mD"
   },
   "source": [
    "<a name=\"task-2\"></a>\n",
    "\n",
    "# Task 2: Classification [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgBfTmgpy08Z"
   },
   "source": [
    "<a name=\"q21\"></a>\n",
    "\n",
    "## 2.1 k-NN Classifier [^](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.855170</td>\n",
       "      <td>15.248290</td>\n",
       "      <td>69.167041</td>\n",
       "      <td>359.534878</td>\n",
       "      <td>0.105488</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.066410</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>...</td>\n",
       "      <td>18.984557</td>\n",
       "      <td>81.443134</td>\n",
       "      <td>466.879302</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.205695</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.335999</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.068958</td>\n",
       "      <td>15.532758</td>\n",
       "      <td>66.130635</td>\n",
       "      <td>330.040665</td>\n",
       "      <td>0.099813</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>0.057583</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>...</td>\n",
       "      <td>22.840293</td>\n",
       "      <td>82.133171</td>\n",
       "      <td>473.367822</td>\n",
       "      <td>0.125478</td>\n",
       "      <td>0.330466</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.312882</td>\n",
       "      <td>0.096158</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.271409</td>\n",
       "      <td>18.100314</td>\n",
       "      <td>78.195610</td>\n",
       "      <td>421.537832</td>\n",
       "      <td>0.105147</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.043317</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.188801</td>\n",
       "      <td>...</td>\n",
       "      <td>26.365608</td>\n",
       "      <td>84.598334</td>\n",
       "      <td>620.586067</td>\n",
       "      <td>0.146766</td>\n",
       "      <td>0.118707</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.291805</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.531733</td>\n",
       "      <td>18.452486</td>\n",
       "      <td>67.227069</td>\n",
       "      <td>340.063033</td>\n",
       "      <td>0.086041</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.203093</td>\n",
       "      <td>...</td>\n",
       "      <td>24.385385</td>\n",
       "      <td>73.296855</td>\n",
       "      <td>429.675600</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.143683</td>\n",
       "      <td>0.177225</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.287749</td>\n",
       "      <td>0.073174</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12.367686</td>\n",
       "      <td>14.399191</td>\n",
       "      <td>80.643814</td>\n",
       "      <td>460.849710</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.195326</td>\n",
       "      <td>...</td>\n",
       "      <td>19.614305</td>\n",
       "      <td>89.910502</td>\n",
       "      <td>472.323112</td>\n",
       "      <td>0.138135</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>0.151098</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.345258</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  n1_radius  n1_texture  n1_perimeter     n1_area  n1_smoothness  \\\n",
       "0           0  10.855170   15.248290     69.167041  359.534878       0.105488   \n",
       "1           1  10.068958   15.532758     66.130635  330.040665       0.099813   \n",
       "2           2  12.271409   18.100314     78.195610  421.537832       0.105147   \n",
       "3           3  10.531733   18.452486     67.227069  340.063033       0.086041   \n",
       "4           4  12.367686   14.399191     80.643814  460.849710       0.106410   \n",
       "\n",
       "   n1_compactness  n1_concavity  n1_concave_points  n1_symmetry  ...  \\\n",
       "0        0.080200      0.066410           0.034194     0.182796  ...   \n",
       "1        0.109540      0.057583           0.023322     0.195650  ...   \n",
       "2        0.095315      0.043317           0.031539     0.188801  ...   \n",
       "3        0.049961      0.049709           0.011046     0.203093  ...   \n",
       "4        0.101420      0.020806           0.021990     0.195326  ...   \n",
       "\n",
       "   n3_texture  n3_perimeter     n3_area  n3_smoothness  n3_compactness  \\\n",
       "0   18.984557     81.443134  466.879302       0.149080        0.200185   \n",
       "1   22.840293     82.133171  473.367822       0.125478        0.330466   \n",
       "2   26.365608     84.598334  620.586067       0.146766        0.118707   \n",
       "3   24.385385     73.296855  429.675600       0.100060        0.143683   \n",
       "4   19.614305     89.910502  472.323112       0.138135        0.276127   \n",
       "\n",
       "   n3_concavity  n3_concave_points  n3_symmetry  n3_fractal_dimension  \\\n",
       "0      0.205695           0.111592     0.335999              0.093477   \n",
       "1      0.283304           0.088021     0.312882              0.096158   \n",
       "2      0.147900           0.050402     0.291805              0.069556   \n",
       "3      0.177225           0.028111     0.287749              0.073174   \n",
       "4      0.151098           0.074396     0.345258              0.095830   \n",
       "\n",
       "   DIAGNOSIS  \n",
       "0          B  \n",
       "1          B  \n",
       "2          B  \n",
       "3          B  \n",
       "4          B  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the sample data set using the pandas read_csv method into a pandas DataFrame.\n",
    "df_tumour_samples = pd.read_csv('./Data/tumour_samples.csv')\n",
    "# Outputing the first 5 samples (rows) to inspect the data.\n",
    "df_tumour_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>n1_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.855170</td>\n",
       "      <td>15.248290</td>\n",
       "      <td>69.167041</td>\n",
       "      <td>359.534878</td>\n",
       "      <td>0.105488</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.066410</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.066968</td>\n",
       "      <td>...</td>\n",
       "      <td>18.984557</td>\n",
       "      <td>81.443134</td>\n",
       "      <td>466.879302</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.205695</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.335999</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.068958</td>\n",
       "      <td>15.532758</td>\n",
       "      <td>66.130635</td>\n",
       "      <td>330.040665</td>\n",
       "      <td>0.099813</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>0.057583</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>0.069572</td>\n",
       "      <td>...</td>\n",
       "      <td>22.840293</td>\n",
       "      <td>82.133171</td>\n",
       "      <td>473.367822</td>\n",
       "      <td>0.125478</td>\n",
       "      <td>0.330466</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.312882</td>\n",
       "      <td>0.096158</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.271409</td>\n",
       "      <td>18.100314</td>\n",
       "      <td>78.195610</td>\n",
       "      <td>421.537832</td>\n",
       "      <td>0.105147</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.043317</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.188801</td>\n",
       "      <td>0.063341</td>\n",
       "      <td>...</td>\n",
       "      <td>26.365608</td>\n",
       "      <td>84.598334</td>\n",
       "      <td>620.586067</td>\n",
       "      <td>0.146766</td>\n",
       "      <td>0.118707</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.291805</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.531733</td>\n",
       "      <td>18.452486</td>\n",
       "      <td>67.227069</td>\n",
       "      <td>340.063033</td>\n",
       "      <td>0.086041</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.203093</td>\n",
       "      <td>0.064948</td>\n",
       "      <td>...</td>\n",
       "      <td>24.385385</td>\n",
       "      <td>73.296855</td>\n",
       "      <td>429.675600</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.143683</td>\n",
       "      <td>0.177225</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.287749</td>\n",
       "      <td>0.073174</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.367686</td>\n",
       "      <td>14.399191</td>\n",
       "      <td>80.643814</td>\n",
       "      <td>460.849710</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.195326</td>\n",
       "      <td>0.064605</td>\n",
       "      <td>...</td>\n",
       "      <td>19.614305</td>\n",
       "      <td>89.910502</td>\n",
       "      <td>472.323112</td>\n",
       "      <td>0.138135</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>0.151098</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.345258</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n1_radius  n1_texture  n1_perimeter     n1_area  n1_smoothness  \\\n",
       "0  10.855170   15.248290     69.167041  359.534878       0.105488   \n",
       "1  10.068958   15.532758     66.130635  330.040665       0.099813   \n",
       "2  12.271409   18.100314     78.195610  421.537832       0.105147   \n",
       "3  10.531733   18.452486     67.227069  340.063033       0.086041   \n",
       "4  12.367686   14.399191     80.643814  460.849710       0.106410   \n",
       "\n",
       "   n1_compactness  n1_concavity  n1_concave_points  n1_symmetry  \\\n",
       "0        0.080200      0.066410           0.034194     0.182796   \n",
       "1        0.109540      0.057583           0.023322     0.195650   \n",
       "2        0.095315      0.043317           0.031539     0.188801   \n",
       "3        0.049961      0.049709           0.011046     0.203093   \n",
       "4        0.101420      0.020806           0.021990     0.195326   \n",
       "\n",
       "   n1_fractal_dimension  ...  n3_texture  n3_perimeter     n3_area  \\\n",
       "0              0.066968  ...   18.984557     81.443134  466.879302   \n",
       "1              0.069572  ...   22.840293     82.133171  473.367822   \n",
       "2              0.063341  ...   26.365608     84.598334  620.586067   \n",
       "3              0.064948  ...   24.385385     73.296855  429.675600   \n",
       "4              0.064605  ...   19.614305     89.910502  472.323112   \n",
       "\n",
       "   n3_smoothness  n3_compactness  n3_concavity  n3_concave_points  \\\n",
       "0       0.149080        0.200185      0.205695           0.111592   \n",
       "1       0.125478        0.330466      0.283304           0.088021   \n",
       "2       0.146766        0.118707      0.147900           0.050402   \n",
       "3       0.100060        0.143683      0.177225           0.028111   \n",
       "4       0.138135        0.276127      0.151098           0.074396   \n",
       "\n",
       "   n3_symmetry  n3_fractal_dimension  DIAGNOSIS  \n",
       "0     0.335999              0.093477          B  \n",
       "1     0.312882              0.096158          B  \n",
       "2     0.291805              0.069556          B  \n",
       "3     0.287749              0.073174          B  \n",
       "4     0.345258              0.095830          B  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain an array of the data-type of all columns\n",
    "if 'Unnamed: 0' in df_tumour_samples.columns:\n",
    "    df_tumour_samples = df_tumour_samples.drop('Unnamed: 0', 1)\n",
    "data_type = np.array([df_tumour_samples.dtypes[i] for i in range(df_tumour_samples.shape[1]-1)])\n",
    "assert(data_type.all() == float)  # we assert that all columns are of type float\n",
    "assert(df_tumour_samples.isnull().values.any() == False)  # we assert that there is no empty / NaN entry in any column\n",
    "df_tumour_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class = df_tumour_samples.iloc[:, 0:30]  # load descriptors into matrix of descriptors\n",
    "y = df_tumour_samples.iloc[:, 30]  # load ground truth target variable vector\n",
    "y = [0 if el == 'B' else 1 for el in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p, q):\n",
    "    return np.sqrt(np.sum((p-q)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(X):\n",
    "    mu = np.mean(X, 0)\n",
    "    sigma = np.std(X, 0)\n",
    "    X_std = (X - mu) / sigma\n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardise(X_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbours(X_train, X_test, k=5, return_distance=False):\n",
    "    n_neighbours = k\n",
    "    dist = []\n",
    "    neigh_ind = []\n",
    "  \n",
    "    # compute distance from each point x_test in X_test to all points in X_train\n",
    "    point_dist = [euclidian_distance(x_test, X_train) for x_test in X_test]\n",
    "    \n",
    "    # determine which k training points are closest to each test point\n",
    "    for row in point_dist:\n",
    "        enum_neigh = enumerate(row)\n",
    "        sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n",
    "\n",
    "        ind_list = [tup[0] for tup in sorted_neigh]\n",
    "        dist_list = [tup[1] for tup in sorted_neigh]\n",
    "\n",
    "        dist.append(dist_list)\n",
    "        neigh_ind.append(ind_list)\n",
    "  \n",
    "    # return distances together with indices of k nearest neighbours\n",
    "    if return_distance:\n",
    "        return np.array(dist), np.array(neigh_ind)\n",
    "  \n",
    "    return np.array(neigh_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, X_test, k=5):\n",
    "    # each of the k neighbours contributes equally to the classification of any data point in X_test  \n",
    "    neighbours = k_neighbours(X_train, X_test, k=k)\n",
    "    # count number of occurences of label with np.bincount and choose the label that has most with np.argmax\n",
    "    y_pred = np.array([np.argmax(np.bincount(y_train[neighbour])) for neighbour in neighbours])\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X_train, y_train, X_test, y_test, k=5):\n",
    "    y_pred = predict(X_train, y_train, X_test, k=k)\n",
    "    return np.float(sum(y_pred==y_test))/ float(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "y_copy = y.copy()\n",
    "\n",
    "X_copy = np.array(X_copy[:-1]) # removing the last element in order to have 5 equal-sized folds (which requires an even number of samples)\n",
    "y_copy = np.array(y_copy[:-1]) # removing the last element in order to have 5 equal-sized folds (which requires an even number of samples)\n",
    "\n",
    "np.random.seed(10)\n",
    "p = np.random.permutation(len(y_copy))\n",
    "X_copy = X_copy[p]\n",
    "y_copy = y_copy[p]\n",
    "\n",
    "# We obtain a list of the 5 index arrays, i.e., the 5 folds\n",
    "folds_indexes = np.split(np.arange(len(y_copy)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(X_train, y_train, folds, k):\n",
    "    scores = []\n",
    "    for i in range(len(folds)):\n",
    "        val_indexes = folds[i]\n",
    "        train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
    "\n",
    "        X_train_i = X_train[train_indexes, :]\n",
    "        y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "        X_val_i = X_train[val_indexes, :]\n",
    "        y_val_i = y_train[val_indexes]\n",
    "\n",
    "        score_i = score(X_train_i, y_train_i, X_val_i, y_val_i, k=k)\n",
    "        scores.append(score_i)\n",
    "\n",
    "    # Return the average score\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_k(X_train, y_train, folds, k_range):\n",
    "    k_scores = np.zeros((len(k_range),))\n",
    "    \n",
    "    for i, k in enumerate(k_range):\n",
    "        k_scores[i] = cross_validation_score(X_train, y_train, folds, k)\n",
    "        print(f'CV_ACC@k={k}: {k_scores[i]:.3f}')\n",
    "\n",
    "    best_k_index = np.argmax(k_scores)\n",
    "    return k_range[best_k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_ACC@k=1: 0.997\n",
      "CV_ACC@k=2: 0.982\n",
      "CV_ACC@k=3: 0.985\n",
      "CV_ACC@k=4: 0.979\n",
      "CV_ACC@k=5: 0.983\n",
      "CV_ACC@k=6: 0.979\n",
      "CV_ACC@k=7: 0.980\n",
      "CV_ACC@k=8: 0.978\n",
      "CV_ACC@k=9: 0.981\n",
      "CV_ACC@k=10: 0.977\n",
      "CV_ACC@k=11: 0.981\n",
      "CV_ACC@k=12: 0.977\n",
      "CV_ACC@k=13: 0.979\n",
      "CV_ACC@k=14: 0.976\n",
      "CV_ACC@k=15: 0.977\n",
      "CV_ACC@k=16: 0.976\n",
      "CV_ACC@k=17: 0.978\n",
      "CV_ACC@k=18: 0.975\n",
      "CV_ACC@k=19: 0.976\n",
      "CV_ACC@k=20: 0.974\n",
      "CV_ACC@k=21: 0.975\n",
      "CV_ACC@k=22: 0.973\n",
      "CV_ACC@k=23: 0.974\n",
      "CV_ACC@k=24: 0.972\n",
      "CV_ACC@k=25: 0.973\n",
      "CV_ACC@k=26: 0.972\n",
      "CV_ACC@k=27: 0.973\n",
      "CV_ACC@k=28: 0.972\n",
      "CV_ACC@k=29: 0.972\n",
      "CV_ACC@k=30: 0.971\n",
      "best_k: 1\n"
     ]
    }
   ],
   "source": [
    "best_k = choose_best_k(X_copy, y_copy, folds_indexes, np.arange(1, 31))\n",
    "print('best_k:', best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set mean accuracy: 1.0\n",
      "Test set mean accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "print('Train set mean accuracy:', score(np.array(X), np.array(y), np.array(X), np.array(y), k=best_k))\n",
    "\n",
    "# Loading the test data set using the pandas read_csv method into a pandas DataFrame.\n",
    "df_tumour_test = pd.read_csv('./Data/tumour_test.csv')\n",
    "if 'Unnamed: 0' in df_tumour_test.columns:\n",
    "    df_tumour_test = df_tumour_test.drop('Unnamed: 0', 1)\n",
    "X_class_test = df_tumour_test.iloc[:, 0:30]  # load descriptors into matrix of descriptors\n",
    "X_test = standardise(X_class_test)\n",
    "y_test = df_tumour_test.iloc[:, 30]  # load ground truth target variable vector\n",
    "y_test = [0 if el == 'B' else 1 for el in y_test]\n",
    "print('Test set mean accuracy:', score(np.array(X), np.array(y), np.array(X_test), np.array(y_test), k=best_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCPlajQOy1ER"
   },
   "source": [
    "<a name=\"q22\"></a>\n",
    "\n",
    "## 2.2 Random Forest [^](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tumour_samples\n",
    "\n",
    "# stacking data X and labels y into one matrix\n",
    "tumour_samples_shuff = df_tumour_samples.iloc[np.random.permutation(len(df_tumour_samples))]\n",
    "\n",
    "X = tumour_samples_shuff[tumour_samples_shuff.columns[:-1]]\n",
    "y = tumour_samples_shuff[tumour_samples_shuff.columns[-1]]\n",
    "y = np.array([0 if el == 'B' else 1 for el in y])\n",
    "y = y.astype(int)\n",
    "\n",
    "training_weights = np.ones_like(y)/len(y)\n",
    "\n",
    "# We need a dictionary indicating whether the column index maps to a \n",
    "# categorical feature or numerical\n",
    "# In this example, all features are numerical (categorical=False)\n",
    "columns_dict = {index: False for index in range(X.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, sample_weights):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy for labels.\n",
    "    Arguments:\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    Returns:\n",
    "        (float): the cross-entropy for y.\n",
    "    \"\"\"\n",
    "\n",
    "    # count different labels in yand store in label_weights\n",
    "    # initialize with zero for each distinct label.\n",
    "    label_weights = {yi: 0 for yi in set(y)}  \n",
    "    for yi, wi in zip(y, sample_weights):\n",
    "        label_weights[yi] += wi\n",
    "\n",
    "    total_weight = sum(label_weights.values())\n",
    "    cross_entropy_value = 0\n",
    "    for label, weight in label_weights.items():\n",
    "        pi_q = (weight / total_weight)\n",
    "        if pi_q == 1:\n",
    "            continue\n",
    "        else:\n",
    "            cross_entropy_value += pi_q * np.log(1 - pi_q)\n",
    "\n",
    "    return cross_entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5327243372511141"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y, training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(X, y, sample_weights, column, value, categorical):\n",
    "    \"\"\"\n",
    "    Return the split of data whose column-th feature:\n",
    "        1. equals value, in case `column` is categorical, or\n",
    "        2. less than value, in case `column` is not categorical (i.e. numerical)\n",
    "\n",
    "    Arguments:\n",
    "        X: training features, of shape (N, D).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        column: the column of the feature for splitting.\n",
    "        value: splitting threshold  the samples \n",
    "        categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "    Returns:\n",
    "        tuple(np.array, np.array): tuple of subsets of X splitted based on column-th value.\n",
    "        tuple(np.array, np.array): tuple of subsets of y splitted based on column-th value.\n",
    "        tuple(np.array, np.array): tuple of subsets of sample weights based on column-th value.\n",
    "    \"\"\" \n",
    "\n",
    "    if categorical:\n",
    "        left_mask =(X[:, column] == value)\n",
    "    else:\n",
    "        left_mask = (X[:, column] < value)\n",
    "  \n",
    "    X_left, X_right = X[left_mask, :], X[~left_mask, :]\n",
    "    y_left, y_right = y[left_mask], y[~left_mask]\n",
    "    w_left, w_right  = sample_weights[left_mask], sample_weights[~left_mask]\n",
    "\n",
    "    return (X_left, X_right), (y_left, y_right), (w_left, w_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_split_value(X, y, sample_weights, column, categorical):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy based on `column` with the split that minimizes the cross-entropy.\n",
    "    Arguments:\n",
    "        X: training features, of shape (N, D).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        column: the column of the feature for calculating. 0 <= column < D\n",
    "        categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "    Returns:\n",
    "        (float, float): the resulted GINI-index and the corresponding value used in splitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_vals = np.unique(X[:, column])\n",
    "    assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
    "\n",
    "    cross_entropy_val, threshold = np.inf, None\n",
    "  \n",
    "    # split the values of i-th feature and calculate the cost \n",
    "    for value in unique_vals:\n",
    "        (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, column, value, categorical)\n",
    "\n",
    "        # if one of the two sides is empty, skip this split.\n",
    "        if len(y_l) == 0 or len(y_r) == 0:\n",
    "            continue\n",
    "    \n",
    "        p_left = sum(w_l)/(sum(w_l) + sum(w_r))\n",
    "        p_right = 1 - p_left\n",
    "        new_cost = p_left * cross_entropy(y_l, w_l) + p_right * cross_entropy(y_r, w_r)\n",
    "        if new_cost < cross_entropy_val:\n",
    "            cross_entropy_val, threshold = new_cost, value\n",
    "    \n",
    "    return cross_entropy_val, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.7982513436984657, 0.08878568428372446)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_split_value(X.to_numpy(), y, training_weights, 4, columns_dict[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_split(X, y, sample_weights, columns_dict):\n",
    "    \"\"\"\n",
    "    Choose the best feature to split according to criterion.\n",
    "    Args:\n",
    "        X: training features, of shape (N, D).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "    Returns:\n",
    "        (int, float): the best feature index and value used in splitting. \n",
    "        If the feature index is None, then no valid split for the current Node.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize `split_column` to None, so if None returned this means there is no valid split at the current node.\n",
    "    min_cross_entropy, split_column, split_val = np.inf, None, 0\n",
    "    for column, categorical in columns_dict.items():\n",
    "        # skip column if samples are not seperable by that column.\n",
    "        if len(np.unique(X[:, column])) < 2:\n",
    "            continue\n",
    "        cross_entropy, val = cross_entropy_split_value(X, y, sample_weights, column, categorical)         \n",
    "        if cross_entropy < min_cross_entropy:\n",
    "            min_cross_entropy, split_column, split_val = cross_entropy, column, val\n",
    "\n",
    "    return split_column, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 95.9483360996496)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate which feature is best\n",
    "cross_entropy_split(X.to_numpy(), y, training_weights, columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(y, sample_weights):\n",
    "    \"\"\"\n",
    "    Return the label which appears the most in y.\n",
    "    Args:\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    Returns:\n",
    "        (int): the majority label\n",
    "    \"\"\"\n",
    "    majority_label = {yi: 0 for yi in set(y)}\n",
    "\n",
    "    for yi, wi in zip(y, sample_weights):\n",
    "        majority_label[yi] += wi\n",
    "    return max(majority_label, key=majority_label.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_vote(y, training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, sample_weights, columns_dict, feature_names, depth,  max_depth=10, min_samples_leaf=2):\n",
    "    \"\"\"Build the decision tree according to the data.\n",
    "    Args:\n",
    "        X: (np.array) training features, of shape (N, D).\n",
    "        y: (np.array) vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        feature_names (list): record the name of features in X in the original dataset.\n",
    "        depth (int): current depth for this node.\n",
    "    Returns:\n",
    "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "          1. 'feature_name': The column name of the split.\n",
    "          2. 'feature_index': The column index of the split.\n",
    "          3. 'value': The value used for the split.\n",
    "          4. 'categorical': indicator for categorical/numerical variables.\n",
    "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
    "          6. 'left': The left sub-tree with the same structure.\n",
    "          7. 'right' The right sub-tree with the same structure.\n",
    "        Example:\n",
    "            mytree = {\n",
    "                'feature_name': 'petal length (cm)',\n",
    "                'feature_index': 2,\n",
    "                'value': 3.0,\n",
    "                'categorical': False,\n",
    "                'majority_label': None,\n",
    "                'left': {\n",
    "                   'feature_name': str,\n",
    "                    'feature_index': int,\n",
    "                    'value': float,\n",
    "                    'categorical': bool,\n",
    "                    'majority_label': None,\n",
    "                    'left': {..etc.},\n",
    "                    'right': {..etc.}\n",
    "                }\n",
    "                'right': {\n",
    "                    'feature_name': str,\n",
    "                    'feature_index': int,\n",
    "                    'value': float,\n",
    "                    'categorical': bool,\n",
    "                    'majority_label': None,\n",
    "                    'left': {..etc.},\n",
    "                    'right': {..etc.}\n",
    "                }\n",
    "            }\n",
    "    \"\"\"\n",
    "    # include a clause for the cases where (i) no feature, (ii) all labels are the same, (iii) depth exceed, or (iv) X is too small\n",
    "    if len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf: \n",
    "        return {'majority_label': majority_vote(y, sample_weights)}\n",
    "  \n",
    "    split_index, split_val = cross_entropy_split(X, y, sample_weights, columns_dict)\n",
    "\n",
    "    # If no valid split at this node, use majority vote.\n",
    "    if split_index is None:\n",
    "        return {'majority_label': majority_vote(y, sample_weights)}\n",
    "\n",
    "    categorical = columns_dict[split_index]\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, split_index, split_val, categorical)\n",
    "    return {\n",
    "        'feature_name': feature_names[split_index],\n",
    "        'feature_index': split_index,\n",
    "        'value': split_val,\n",
    "        'categorical': categorical,\n",
    "        'majority_label': None,\n",
    "        'left': build_tree(X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
    "        'right': build_tree(X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y,  columns_dict, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Build the decision tree according to the training data.\n",
    "    Args:\n",
    "        X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n",
    "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    \"\"\"\n",
    "    if sample_weights is None:\n",
    "        # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X = X.to_numpy()\n",
    "    y = y\n",
    "    return build_tree(X, y, sample_weights, columns_dict, feature_names, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 4 s, total: 15 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tree = train(X, y, columns_dict)\n",
    "# print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x):\n",
    "    \"\"\"\n",
    "    Classify a single sample with the fitted decision tree.\n",
    "    Args:\n",
    "        x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
    "    Returns:\n",
    "        (int): predicted testing sample label.\n",
    "    \"\"\"\n",
    "    if tree['majority_label'] is not None: \n",
    "        return tree['majority_label']\n",
    "\n",
    "    elif tree['categorical']:\n",
    "        if x[tree['feature_index']] == tree['value']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'], x)\n",
    "\n",
    "    else:\n",
    "        if x[tree['feature_index']] < tree['value']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, X):\n",
    "    \"\"\"\n",
    "    Predict classification results for X.\n",
    "    Args:\n",
    "        X: (pd.Dataframe) testing sample features, of shape (N, D).\n",
    "    Returns:\n",
    "        (np.array): predicted testing sample labels, of shape (N,).\n",
    "    \"\"\"\n",
    "    if len(X.shape) == 1:\n",
    "        return classify(tree, X)\n",
    "    else:\n",
    "        return np.array([classify(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_score(tree, X_test, y_test):\n",
    "    y_pred = predict(tree, X_test)\n",
    "    return np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, tree_score(\u001b[43mtree\u001b[49m, X\u001b[38;5;241m.\u001b[39mto_numpy(), y))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training accuracy:', tree_score(tree, X.to_numpy(), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_split_rf(n_features, X, y, sample_weights, columns_dict):\n",
    "    \"\"\"\n",
    "    Choose the best feature to split according to criterion.\n",
    "    Args:\n",
    "        n_features: number of sampled features.\n",
    "        X: training features, of shape (N, D).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "    Returns:\n",
    "        (float, int, float): the minimized gini-index, the best feature index and value used in splitting.\n",
    "    \"\"\"\n",
    "    columns = np.random.choice(list(columns_dict.keys()), n_features, replace=False)\n",
    "    columns_dict = {c: columns_dict[c] for c in columns}\n",
    "\n",
    "    min_cross_entropy, split_column, split_val = np.inf, 0, 0\n",
    "    for column, categorical in columns_dict.items():\n",
    "        # skip column if samples are not seperable by that column.\n",
    "        if len(np.unique(X[:, column])) < 2:\n",
    "            continue\n",
    "   \n",
    "        # search for the best splitting value for the given column.\n",
    "        cross_entropy, val = cross_entropy_split_value(X, y, sample_weights, column, categorical)      \n",
    "        if cross_entropy < min_cross_entropy:\n",
    "            min_cross_entropy, split_column, split_val = cross_entropy, column, val\n",
    "\n",
    "    return min_cross_entropy, split_column, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_rf(n_features, X, y, sample_weights, columns_dict, feature_names, depth,  max_depth=10, min_samples_leaf=2):\n",
    "    \"\"\"Build the decision tree according to the data.\n",
    "    Args:\n",
    "        X: (np.array) training features, of shape (N, D).\n",
    "        y: (np.array) vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        feature_names (list): record the name of features in X in the original dataset.\n",
    "        depth (int): current depth for this node.\n",
    "    Returns:\n",
    "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "          1. 'feature_name': The column name of the split.\n",
    "          2. 'feature_index': The column index of the split.\n",
    "          3. 'value': The value used for the split.\n",
    "          4. 'categorical': indicator for categorical/numerical variables.\n",
    "          5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
    "          6. 'left': The left sub-tree with the same structure.\n",
    "          7. 'right' The right sub-tree with the same structure.\n",
    "        Example:\n",
    "            mytree = {\n",
    "                'feature_name': 'petal length (cm)',\n",
    "                'feature_index': 2,\n",
    "                'value': 3.0,\n",
    "                'categorical': False,\n",
    "                'majority_label': None,\n",
    "                'left': {\n",
    "                    'feature_name': str,\n",
    "                    'feature_index': int,\n",
    "                    'value': float,\n",
    "                    'categorical': bool,\n",
    "                    'majority_label': None,\n",
    "                    'left': {..etc.},\n",
    "                    'right': {..etc.}\n",
    "                }\n",
    "                'right': {\n",
    "                    'feature_name': str,\n",
    "                    'feature_index': int,\n",
    "                    'value': float,\n",
    "                    'categorical': bool,\n",
    "                    'majority_label': None,\n",
    "                    'left': {..etc.},\n",
    "                    'right': {..etc.}\n",
    "                }\n",
    "            }\n",
    "    \"\"\"\n",
    "    # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
    "    if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf: \n",
    "        return {'majority_label': majority_vote(y, sample_weights)}\n",
    "  \n",
    "    else:\n",
    "        cross_entropy, split_index, split_val = cross_entropy_split_rf(n_features, X, y, sample_weights, columns_dict)\n",
    "    \n",
    "        # If cross_entropy is infinity, it means that samples are not seperable by the sampled features.\n",
    "        if cross_entropy == np.inf:\n",
    "            return {'majority_label': majority_vote(y, sample_weights)}\n",
    "        categorical = columns_dict[split_index]\n",
    "        (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, split_index, split_val, categorical)\n",
    "        return {\n",
    "            'feature_name': feature_names[split_index],\n",
    "            'feature_index': split_index,\n",
    "            'value': split_val,\n",
    "            'categorical': categorical,\n",
    "            'majority_label': None,\n",
    "            'left': build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
    "            'right': build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(B, n_features, X, y,  columns_dict, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Build the decision tree according to the training data.\n",
    "    Args:\n",
    "        B: number of decision trees.\n",
    "        X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n",
    "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    \"\"\"\n",
    "    if sample_weights is None:\n",
    "        # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X = X.to_numpy()\n",
    "    if type(y).__module__ != np.__name__:\n",
    "        y = y.to_numpy()\n",
    "    N = X.shape[0]\n",
    "    training_indices = np.arange(N)\n",
    "    trees = []\n",
    "\n",
    "    for _ in range(B):\n",
    "        sample = np.random.choice(training_indices, N, replace=True)\n",
    "        X_sample = X[sample, :]\n",
    "        y_sample = y[sample]\n",
    "        w_sample = sample_weights[sample]\n",
    "        tree = build_tree_rf(n_features, X_sample, y_sample, w_sample, columns_dict, feature_names, depth=1)\n",
    "        trees.append(tree)\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(rf, X):\n",
    "    \"\"\"\n",
    "    Predict classification results for X.\n",
    "    Args:\n",
    "        rf: A trained random forest through train_rf function.\n",
    "        X: (pd.Dataframe) testing sample features, of shape (N, D).\n",
    "    Returns:\n",
    "        (np.array): predicted testing sample labels, of shape (N,).\n",
    "    \"\"\"\n",
    "\n",
    "    def aggregate(decisions):\n",
    "        count = defaultdict(int)\n",
    "        for decision in decisions:\n",
    "            count[decision] += 1\n",
    "        return max(count, key=count.get)\n",
    "\n",
    "    if len(X.shape) == 1:\n",
    "        return aggregate([classify(tree, X) for tree in rf])\n",
    "    else:\n",
    "        return np.array([aggregate([classify(tree, x) for tree in rf]) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_score(rf, X_test, y_test):\n",
    "    y_pred = predict_rf(rf, X_test)\n",
    "    return np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mtrain_rf\u001b[0;34m(B, n_features, X, y, columns_dict, sample_weights)\u001b[0m\n\u001b[1;32m     29\u001b[0m     y_sample \u001b[38;5;241m=\u001b[39m y[sample]\n\u001b[1;32m     30\u001b[0m     w_sample \u001b[38;5;241m=\u001b[39m sample_weights[sample]\n\u001b[0;32m---> 31\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     trees\u001b[38;5;241m.\u001b[39mappend(tree)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trees\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf)\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf)\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf)\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf),\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf),\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf)\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf)\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     56\u001b[0m categorical \u001b[38;5;241m=\u001b[39m columns_dict[split_index]\n\u001b[1;32m     57\u001b[0m (X_l, X_r), (y_l, y_r), (w_l, w_r) \u001b[38;5;241m=\u001b[39m split_samples(X, y, sample_weights, split_index, split_val, categorical)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names[split_index],\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m: split_index,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: split_val,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m: categorical,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbuild_tree_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth, min_samples_leaf)\n\u001b[1;32m     66\u001b[0m }\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mbuild_tree_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict, feature_names, depth, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority_label\u001b[39m\u001b[38;5;124m'\u001b[39m: majority_vote(y, sample_weights)}\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     cross_entropy, split_index, split_val \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_split_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# If cross_entropy is infinity, it means that samples are not seperable by the sampled features.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cross_entropy \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39minf:\n",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36mcross_entropy_split_rf\u001b[0;34m(n_features, X, y, sample_weights, columns_dict)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# search for the best splitting value for the given column.\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cross_entropy, val \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_split_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cross_entropy \u001b[38;5;241m<\u001b[39m min_cross_entropy:\n\u001b[1;32m     25\u001b[0m     min_cross_entropy, split_column, split_val \u001b[38;5;241m=\u001b[39m cross_entropy, column, val\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mcross_entropy_split_value\u001b[0;34m(X, y, sample_weights, column, categorical)\u001b[0m\n\u001b[1;32m     27\u001b[0m p_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(w_l)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28msum\u001b[39m(w_l) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(w_r))\n\u001b[1;32m     28\u001b[0m p_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p_left\n\u001b[0;32m---> 29\u001b[0m new_cost \u001b[38;5;241m=\u001b[39m p_left \u001b[38;5;241m*\u001b[39m cross_entropy(y_l, w_l) \u001b[38;5;241m+\u001b[39m p_right \u001b[38;5;241m*\u001b[39m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_cost \u001b[38;5;241m<\u001b[39m cross_entropy_val:\n\u001b[1;32m     31\u001b[0m     cross_entropy_val, threshold \u001b[38;5;241m=\u001b[39m new_cost, value\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(y, sample_weights)\u001b[0m\n\u001b[1;32m     13\u001b[0m label_weights \u001b[38;5;241m=\u001b[39m {yi: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(y)}  \n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m yi, wi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(y, sample_weights):\n\u001b[0;32m---> 15\u001b[0m     label_weights[yi] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m wi\n\u001b[1;32m     17\u001b[0m total_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(label_weights\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     18\u001b[0m cross_entropy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_features = X.shape[1] // 3\n",
    "B = 50\n",
    "# fit the random forest with training data\n",
    "rf = train_rf(B, n_features, X, y, columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_score(rf, X.to_numpy(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfN9uz1qy1L4"
   },
   "source": [
    "<a name=\"q23\"></a>\n",
    "\n",
    "## 2.3 Support Vector Machine (SVM) [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlwygRTDy1fb"
   },
   "source": [
    "<a name=\"task-3\"></a>\n",
    "\n",
    "# Task 3: Mastery Component [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdYowG2Ny1qx"
   },
   "source": [
    "<a name=\"q31\"></a>\n",
    "\n",
    "## 3.1 Logistic Regression and Bagging [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XbuLm6qy100"
   },
   "source": [
    "<a name=\"q32\"></a>\n",
    "\n",
    "## 3.2 Kernelised SVM Classifier [^](#outline)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SurnameCID_CW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
